{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, w, b):\n",
    "        self.params = [w, b]\n",
    "        self.grads = [np.zeros_like(w), np.zeros_like(b)]\n",
    "        self.x, self.x_shape = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_shape = x.shape\n",
    "        self.x = x.reshape(x.shape[0], -1)\n",
    "        return np.dot(self.x, self.params[0]) + self.params[1]\n",
    "\n",
    "    def backward(self, dout):\n",
    "        self.grads[0][...] = np.dot(self.x.T, dout)\n",
    "        self.grads[1][...] = np.sum(dout, axis=0)\n",
    "        dx = np.dot(dout, self.params[0].T)\n",
    "        return dx.reshape(*self.x_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y, self.t = None, None\n",
    "        self.delta = 1e-7\n",
    "\n",
    "    def softmax(self, x, T=1):\n",
    "        # Tは温度パラメータ\n",
    "        if x.ndim == 2:\n",
    "            x -= x.max(axis=1, keepdims=True)\n",
    "            x = np.exp(x / T)\n",
    "            x /= x.sum(axis=1, keepdims=True)\n",
    "        elif x.ndim == 1:\n",
    "            x -= np.max(x)\n",
    "            x = np.exp(x / T)\n",
    "            x /= np.sum(x)\n",
    "        return x\n",
    "\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(1, y.size)\n",
    "        if t.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "        if t.size == y.size:\n",
    "            # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
    "            t = t.argmax(axis=1)\n",
    "        return -np.sum(np.log(y[np.arange(y.shape[0]), t] + self.delta)) / y.shape[0]\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "        if self.t.size == self.y.size:\n",
    "            # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        return self.cross_entropy_error(self.y, self.t)\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size:\n",
    "            return (self.y - self.t) / size\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(size), self.t] -= 1\n",
    "        return dx / size\n",
    "        #dx *= dout\n",
    "        #dx /= self.t.shape[0]\n",
    "        #return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr, self.momentum = lr, momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.v = None\n",
    "        self.delta = 1e-7\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] += grads[i] ** 2\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.v[i]) + self.delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay = 0.99):\n",
    "        self.lr, self.decay = lr, decay\n",
    "        self.v = None\n",
    "        self.delta = 1e-7\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] *= self.decay\n",
    "            self.v[i] += (1 - self.decay) * grads[i] ** 2\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.v[i]) + self.delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, ratio=0.5):\n",
    "        self.ratio = ratio\n",
    "        self.mask = None\n",
    "        self.params, self.grads = [], []\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        if train:\n",
    "            self.mask = np.random.rand(*x.shape) > self.ratio\n",
    "            return x * self.mask\n",
    "        self.mask = np.ones_like(*x.shape, dtype=bool)\n",
    "        return x * (1.0 - self.ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, gamma, beta, momentum=0.9, mean=None, var=None):\n",
    "        self.params, self.grads = [], []\n",
    "        self.gamma, self.beta, self.momentum, self.mean, self.var = gamma, beta, momentum, mean, var\n",
    "        self.input_shape, self.xc, self.std, self.dgamma, self.dbeta = None, None, None, None, None\n",
    "        self.delta = 1e-6\n",
    "\n",
    "    def __forward(self, x, train):\n",
    "        if self.mean is None:\n",
    "            _, D = x.shape\n",
    "            self.mean, self.var = np.zeros(D), np.zeros(D)\n",
    "        if train:\n",
    "            mu = x.mean(axis=0)\n",
    "            self.xc = x - mu\n",
    "            var = np.mean(self.xc ** 2, axis=0)\n",
    "            self.std = np.sqrt(var + self.delta)\n",
    "            self.xn = xn = self.xc / self.std\n",
    "            self.mean = self.momentum * self.mean + (1 - self.momentum) * mu\n",
    "            self.var = self.momentum * self.var + (1 - self.momentum) * var\n",
    "        else:\n",
    "            xn = (x - self.mean) / np.sqrt(self.var + self.delta)\n",
    "        return self.gamma * xn + self.beta\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, _, _, _ = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "        out = self.__forward(x, train)\n",
    "        return out.reshape(*x.shape)\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        self.dbeta = dout.sum(axis=0)\n",
    "        self.dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std **2), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.input_shape[0]) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        return dxc - dmu / self.input_shape[0]\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, _, _, _ = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "        dx = self.__backward(dout)\n",
    "        return dx.reshape(*self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads, self.layers = [], [], []\n",
    "        self.loss = None\n",
    "    \n",
    "    def append(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params += layer.params\n",
    "        self.grads += layer.grads\n",
    "    \n",
    "    def append_loss(self, layer):\n",
    "        self.loss = layer\n",
    "    \n",
    "    def predict(self, x, train=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout) or isinstance(layer, BatchNormalization):\n",
    "                x = layer.forward(x, train)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, t, train=True):\n",
    "        y = self.predict(x, train)\n",
    "        return self.loss.forward(y, t)\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def summary(self):\n",
    "        print('-' * 50)\n",
    "        for layer in self.layers:\n",
    "            print(type(layer))\n",
    "            for param in layer.params:\n",
    "                print(param.shape)\n",
    "        print(type(self.loss))\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model, self.optimizer = model, optimizer\n",
    "    \n",
    "    def fit(self, x, t, epoch_size, batch_size):\n",
    "        size = len(x)\n",
    "        iters = size // batch_size\n",
    "        for ep in range(epoch_size):\n",
    "            total_loss = 0\n",
    "            loss_count = 0\n",
    "            accuracy = 0\n",
    "            for it in range(iters):\n",
    "                mask = np.random.choice(size, batch_size)\n",
    "                x_batch, t_batch = x[mask], t[mask]\n",
    "                y = self.model.predict(x_batch, True)\n",
    "                loss = self.model.loss.forward(y, t_batch)\n",
    "                #if student is not None:\n",
    "                #    p = student.loss.softmax(y, T)\n",
    "                #    q = student.loss.softmax(student.predict(x_batch, True), T)\n",
    "                #    student.backward()\n",
    "                    \n",
    "                #loss = self.model.forward(x_batch, t_batch)\n",
    "                self.model.backward()\n",
    "                self.optimizer.update(self.model.params, self.model.grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "                if t_batch.ndim != 1: t_batch = np.argmax(t_batch, axis=1)\n",
    "                accuracy += np.sum(np.argmax(y, axis=1) == t_batch) / float(x_batch.shape[0])\n",
    "            print('loss %.2f | accuracy %.4f' %(total_loss / loss_count, accuracy / loss_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データセット読み込み\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from deep_learning_from_scratch.dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(count, typ=0):\n",
    "    if typ == 0:\n",
    "        return np.sqrt(2.0 / count) #He\n",
    "    return np.sqrt(1.0 / count) # Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習\n",
    "m1 = Model()\n",
    "m1.append(Affine(scale(784) * np.random.randn(784, 100), np.zeros(100)))\n",
    "m1.append(BatchNormalization(np.ones(100), np.zeros(100)))\n",
    "m1.append(Relu())\n",
    "m1.append(Affine(scale(100) * np.random.randn(100, 10), np.zeros(10)))\n",
    "m1.append_loss(SoftmaxWithLoss())\n",
    "m1.summary()\n",
    "o1 = SGD(lr=0.1)\n",
    "t1 = Trainer(m1, o1)\n",
    "t1.fit(x_train, t_train, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テスト\n",
    "ret = []\n",
    "for i in range(len(x_test)):\n",
    "    score = m1.predict(x_test[[i]])\n",
    "    ret.extend(np.argmax(score, axis=1) == np.argmax(t_test[[i]], axis=1))\n",
    "print('accuracy %.4f' %(ret.count(True)/ len(ret)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNUtil:\n",
    "    def __init__(self, fh, fw, stride=1, pad=0):\n",
    "        self.fh, self.fw, self.stride, self.pad = fh, fw, stride, pad\n",
    "        self.N, self.C, self.H, self.W = None, None, None, None\n",
    "        self.oh, self.ow = None, None\n",
    "    \n",
    "    def im2col(self, im):\n",
    "        self.N, self.C, self.H, self.W = im.shape\n",
    "        self.oh = (self.H + 2 * self.pad - self.fh) // self.stride + 1\n",
    "        self.ow = (self.W + 2 * self.pad - self.fw) // self.stride + 1\n",
    "        img = np.pad(im, [(0,0), (0,0), (self.pad, self.pad), (self.pad, self.pad)], 'constant')\n",
    "        col = np.zeros((self.N, self.C, self.fh, self.fw, self.oh, self.ow))\n",
    "        for h in range(self.fh):\n",
    "            h_max = h + self.stride * self.oh\n",
    "            for w in range(self.fw):\n",
    "                w_max = w + self.stride * self.ow\n",
    "                col[:, :, h, w, :, :] = img[:, :, h: h_max: self.stride, w: w_max: self.stride]\n",
    "        return col.transpose(0, 4, 5, 1, 2, 3).reshape(self.N * self.oh * self.ow, -1)\n",
    "    \n",
    "    def col2im(self, col):\n",
    "        col = col.reshape(self.N, self.oh, self.ow, self.C, self.fh, self.fw).transpose(0, 3, 4, 5, 1, 2)\n",
    "        img = np.zeros((self.N, self.C, self.H + 2 * self.pad + self.stride - 1, self.W + 2 * self.pad + self.stride - 1))\n",
    "        for h in range(self.fh):\n",
    "            h_max = h + self.stride * self.oh\n",
    "            for w in range(self.fw):\n",
    "                w_max = w + self.stride * self.ow\n",
    "                img[:, :, h: h_max: self.stride, w: w_max: self.stride] += col[:, :, h, w, :, :]\n",
    "        return img[:, :, self.pad: self.H + self.pad, self.pad: self.W + self.pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, w, b, stride=1, pad=0):\n",
    "        self.params = [w, b]\n",
    "        self.grads = [np.zeros_like(w), np.zeros_like(b)]\n",
    "        self.col, self.col_w = None, None\n",
    "        _, _, FH, FW = w.shape\n",
    "        self.u = CNNUtil(FH, FW, stride, pad)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w, b = self.params\n",
    "        FN, C, FH, FW = w.shape\n",
    "        N, _, H, W = x.shape\n",
    "        self.col = self.u.im2col(x)\n",
    "        self.col_w = w.reshape(FN, -1)\n",
    "        out = np.dot(self.col, self.col_w.T) + b\n",
    "        return out.reshape(N, self.u.oh, self.u.ow, -1).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        w, _ = self.params\n",
    "        FN, C, FH, FW = w.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        dw = np.dot(self.col.T, dout)\n",
    "        self.grads[0][...] = dw.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        self.grads[1][...] = np.sum(dout, axis=0)\n",
    "        return  self.u.col2im(np.dot(dout, self.col_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, ph=2, pw=2, stride=2, pad=0):\n",
    "        self.params, self.grads = [], []\n",
    "        self.ph, self.pw, self.stride, self.pad = ph, pw, stride, pad\n",
    "        self.argmax = None\n",
    "        self.u = CNNUtil(ph, pw, stride, pad)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        col = self.u.im2col(x).reshape(-1, self.ph * self.pw)\n",
    "        self.argmax = np.argmax(col, axis=1)\n",
    "        oh = (H - self.ph) // self.stride + 1\n",
    "        ow = (W - self.pw) // self.stride + 1\n",
    "        return np.max(col, axis=1).reshape(N, oh, ow, C).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        size = self.ph * self.pw\n",
    "        dmax = np.zeros((dout.size, size))\n",
    "        dmax[np.arange(self.argmax.size), self.argmax.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (size,))\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        return self.u.col2im(dcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データセット読み込み\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from deep_learning_from_scratch.dataset.mnist import load_mnist\n",
    "(cx_train, ct_train), (cx_test, ct_test) = load_mnist(flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習\n",
    "m2 = Model()\n",
    "m2.append(Convolution(scale(1*3*3) * np.random.randn(8, 1, 3, 3), np.zeros(8)))\n",
    "m2.append(Relu())\n",
    "m2.append(Convolution(scale(8*3*3) * np.random.randn(8, 8, 3, 3), np.zeros(8)))\n",
    "m2.append(Relu())\n",
    "m2.append(Pooling())\n",
    "m2.append(Convolution(scale(8*3*3) * np.random.randn(8, 8, 3, 3), np.zeros(8)))\n",
    "m2.append(Relu())\n",
    "m2.append(Convolution(scale(8*3*3) * np.random.randn(8, 8, 3, 3), np.zeros(8)))\n",
    "m2.append(Relu())\n",
    "m2.append(Pooling())\n",
    "m2.append(Affine(scale(8*4*4) * np.random.randn(8*4*4, 50), np.zeros(50)))\n",
    "m2.append(Relu())\n",
    "#m2.append(Dropout())\n",
    "m2.append(Affine(scale(50) * np.random.randn(50, 10), np.zeros(10)))\n",
    "#m2.append(Dropout())\n",
    "m2.append_loss(SoftmaxWithLoss())\n",
    "m2.summary()\n",
    "o2 = SGD(lr=0.1)\n",
    "t2 = Trainer(m2, o2)\n",
    "t2.fit(cx_train, ct_train, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テスト\n",
    "ret = []\n",
    "for i in range(len(cx_test)):\n",
    "    score = m2.predict(cx_test[[i]])\n",
    "    ret.extend(np.argmax(score, axis=1) == ct_test[i])\n",
    "print('accuracy %.4f' %(ret.count(True)/ len(ret)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shape(x, layers):\n",
    "    print(x.shape)\n",
    "    for layer in layers:\n",
    "        x = layer.forward(x)\n",
    "        print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
